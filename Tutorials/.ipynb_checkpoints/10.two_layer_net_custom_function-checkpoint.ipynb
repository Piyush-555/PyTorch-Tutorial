{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Defining New autograd Functions\n",
    "----------------------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation computes the forward pass using operations on PyTorch\n",
    "Variables, and uses PyTorch autograd to compute gradients.\n",
    "\n",
    "In this implementation we implement our own custom autograd function to perform\n",
    "the ReLU function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 42057624.0\n",
      "1 41157980.0\n",
      "2 38808380.0\n",
      "3 30229334.0\n",
      "4 18554472.0\n",
      "5 9621538.0\n",
      "6 4931002.5\n",
      "7 2856007.0\n",
      "8 1926771.625\n",
      "9 1450763.0\n",
      "10 1161231.125\n",
      "11 959224.5\n",
      "12 805862.5625\n",
      "13 684087.625\n",
      "14 585033.0625\n",
      "15 503369.3125\n",
      "16 435371.59375\n",
      "17 378312.40625\n",
      "18 330138.53125\n",
      "19 289265.15625\n",
      "20 254411.703125\n",
      "21 224492.3125\n",
      "22 198684.609375\n",
      "23 176357.078125\n",
      "24 156968.765625\n",
      "25 140073.453125\n",
      "26 125316.734375\n",
      "27 112384.96875\n",
      "28 101011.2421875\n",
      "29 90980.5390625\n",
      "30 82110.4921875\n",
      "31 74237.546875\n",
      "32 67236.9140625\n",
      "33 60995.9296875\n",
      "34 55423.4140625\n",
      "35 50434.875\n",
      "36 45960.8984375\n",
      "37 41940.25\n",
      "38 38321.91796875\n",
      "39 35058.03515625\n",
      "40 32109.81640625\n",
      "41 29445.36328125\n",
      "42 27035.888671875\n",
      "43 24855.162109375\n",
      "44 22871.451171875\n",
      "45 21067.01953125\n",
      "46 19423.40625\n",
      "47 17925.0625\n",
      "48 16555.56640625\n",
      "49 15303.4560546875\n",
      "50 14157.205078125\n",
      "51 13107.7880859375\n",
      "52 12144.8076171875\n",
      "53 11260.240234375\n",
      "54 10447.50390625\n",
      "55 9700.0859375\n",
      "56 9013.1083984375\n",
      "57 8381.0751953125\n",
      "58 7798.0283203125\n",
      "59 7259.80908203125\n",
      "60 6762.80859375\n",
      "61 6303.3291015625\n",
      "62 5878.1298828125\n",
      "63 5484.794921875\n",
      "64 5120.50048828125\n",
      "65 4782.68603515625\n",
      "66 4469.22412109375\n",
      "67 4178.1044921875\n",
      "68 3907.66455078125\n",
      "69 3656.309814453125\n",
      "70 3422.575439453125\n",
      "71 3205.089111328125\n",
      "72 3002.625\n",
      "73 2814.011962890625\n",
      "74 2638.1552734375\n",
      "75 2474.200927734375\n",
      "76 2321.292724609375\n",
      "77 2178.601806640625\n",
      "78 2045.9622802734375\n",
      "79 1922.110107421875\n",
      "80 1806.3466796875\n",
      "81 1698.1807861328125\n",
      "82 1596.93798828125\n",
      "83 1502.198974609375\n",
      "84 1413.5577392578125\n",
      "85 1330.5250244140625\n",
      "86 1252.754150390625\n",
      "87 1179.848388671875\n",
      "88 1111.4869384765625\n",
      "89 1047.3602294921875\n",
      "90 987.2030029296875\n",
      "91 930.771728515625\n",
      "92 877.7721557617188\n",
      "93 828.00341796875\n",
      "94 781.2472534179688\n",
      "95 737.3013916015625\n",
      "96 696.001220703125\n",
      "97 657.1679077148438\n",
      "98 620.6343383789062\n",
      "99 586.260009765625\n",
      "100 553.9164428710938\n",
      "101 523.4683227539062\n",
      "102 494.7928161621094\n",
      "103 467.78948974609375\n",
      "104 442.3457336425781\n",
      "105 418.37371826171875\n",
      "106 395.769287109375\n",
      "107 374.46063232421875\n",
      "108 354.3716125488281\n",
      "109 335.4214172363281\n",
      "110 317.5429992675781\n",
      "111 300.66851806640625\n",
      "112 284.7408752441406\n",
      "113 269.6998596191406\n",
      "114 255.50733947753906\n",
      "115 242.0933837890625\n",
      "116 229.4236297607422\n",
      "117 217.45286560058594\n",
      "118 206.13507080078125\n",
      "119 195.43783569335938\n",
      "120 185.3250732421875\n",
      "121 175.76133728027344\n",
      "122 166.71409606933594\n",
      "123 158.1575927734375\n",
      "124 150.061279296875\n",
      "125 142.39791870117188\n",
      "126 135.14308166503906\n",
      "127 128.2759552001953\n",
      "128 121.77462768554688\n",
      "129 115.61761474609375\n",
      "130 109.78709411621094\n",
      "131 104.26008605957031\n",
      "132 99.02571868896484\n",
      "133 94.06543731689453\n",
      "134 89.36410522460938\n",
      "135 84.90713500976562\n",
      "136 80.6824951171875\n",
      "137 76.67584991455078\n",
      "138 72.87649536132812\n",
      "139 69.27404022216797\n",
      "140 65.85778045654297\n",
      "141 62.61668395996094\n",
      "142 59.5398063659668\n",
      "143 56.619564056396484\n",
      "144 53.847537994384766\n",
      "145 51.21682357788086\n",
      "146 48.71962356567383\n",
      "147 46.348175048828125\n",
      "148 44.09654235839844\n",
      "149 41.958309173583984\n",
      "150 39.92732238769531\n",
      "151 37.997764587402344\n",
      "152 36.1646842956543\n",
      "153 34.422943115234375\n",
      "154 32.76774597167969\n",
      "155 31.194740295410156\n",
      "156 29.70050621032715\n",
      "157 28.278820037841797\n",
      "158 26.927858352661133\n",
      "159 25.644105911254883\n",
      "160 24.42281723022461\n",
      "161 23.26142120361328\n",
      "162 22.157854080200195\n",
      "163 21.107749938964844\n",
      "164 20.1085205078125\n",
      "165 19.158132553100586\n",
      "166 18.25370216369629\n",
      "167 17.392791748046875\n",
      "168 16.57447052001953\n",
      "169 15.795381546020508\n",
      "170 15.053411483764648\n",
      "171 14.347366333007812\n",
      "172 13.675353050231934\n",
      "173 13.035563468933105\n",
      "174 12.426871299743652\n",
      "175 11.84675121307373\n",
      "176 11.294665336608887\n",
      "177 10.769041061401367\n",
      "178 10.268414497375488\n",
      "179 9.791297912597656\n",
      "180 9.337081909179688\n",
      "181 8.904391288757324\n",
      "182 8.492226600646973\n",
      "183 8.100037574768066\n",
      "184 7.725729465484619\n",
      "185 7.369068622589111\n",
      "186 7.029426574707031\n",
      "187 6.705803394317627\n",
      "188 6.397449016571045\n",
      "189 6.103261470794678\n",
      "190 5.823115825653076\n",
      "191 5.556210041046143\n",
      "192 5.301338195800781\n",
      "193 5.058393955230713\n",
      "194 4.827040195465088\n",
      "195 4.606480121612549\n",
      "196 4.3960418701171875\n",
      "197 4.195362091064453\n",
      "198 4.004183769226074\n",
      "199 3.8216402530670166\n",
      "200 3.647803783416748\n",
      "201 3.482105016708374\n",
      "202 3.3239901065826416\n",
      "203 3.1729791164398193\n",
      "204 3.0289666652679443\n",
      "205 2.8918092250823975\n",
      "206 2.760629653930664\n",
      "207 2.635542631149292\n",
      "208 2.516369104385376\n",
      "209 2.4025745391845703\n",
      "210 2.2941040992736816\n",
      "211 2.190545082092285\n",
      "212 2.0918350219726562\n",
      "213 1.9975897073745728\n",
      "214 1.9074733257293701\n",
      "215 1.8216309547424316\n",
      "216 1.7397087812423706\n",
      "217 1.6614032983779907\n",
      "218 1.5869282484054565\n",
      "219 1.5156351327896118\n",
      "220 1.4474798440933228\n",
      "221 1.3826287984848022\n",
      "222 1.3206171989440918\n",
      "223 1.2614104747772217\n",
      "224 1.20491361618042\n",
      "225 1.1510577201843262\n",
      "226 1.0995088815689087\n",
      "227 1.0503215789794922\n",
      "228 1.0033307075500488\n",
      "229 0.9585157632827759\n",
      "230 0.9158044457435608\n",
      "231 0.8749070167541504\n",
      "232 0.835902988910675\n",
      "233 0.7986125946044922\n",
      "234 0.7630467414855957\n",
      "235 0.729100227355957\n",
      "236 0.6966179609298706\n",
      "237 0.6656447052955627\n",
      "238 0.6360065340995789\n",
      "239 0.6077935099601746\n",
      "240 0.580733597278595\n",
      "241 0.5549492239952087\n",
      "242 0.5302790403366089\n",
      "243 0.5067575573921204\n",
      "244 0.4842328131198883\n",
      "245 0.46273645758628845\n",
      "246 0.44228416681289673\n",
      "247 0.42270100116729736\n",
      "248 0.4039916694164276\n",
      "249 0.38605359196662903\n",
      "250 0.36897850036621094\n",
      "251 0.352673202753067\n",
      "252 0.3370598554611206\n",
      "253 0.32217851281166077\n",
      "254 0.30791476368904114\n",
      "255 0.29435205459594727\n",
      "256 0.2813175618648529\n",
      "257 0.2689530849456787\n",
      "258 0.2570597231388092\n",
      "259 0.24569611251354218\n",
      "260 0.23488470911979675\n",
      "261 0.22452224791049957\n",
      "262 0.2146335244178772\n",
      "263 0.205154150724411\n",
      "264 0.19610385596752167\n",
      "265 0.18749494850635529\n",
      "266 0.17924541234970093\n",
      "267 0.17136569321155548\n",
      "268 0.16379816830158234\n",
      "269 0.15660962462425232\n",
      "270 0.14972816407680511\n",
      "271 0.14314521849155426\n",
      "272 0.13684406876564026\n",
      "273 0.1308440864086151\n",
      "274 0.12508922815322876\n",
      "275 0.11961231380701065\n",
      "276 0.11435232311487198\n",
      "277 0.10931956022977829\n",
      "278 0.10452151298522949\n",
      "279 0.09994582086801529\n",
      "280 0.09552043676376343\n",
      "281 0.09135175496339798\n",
      "282 0.08735902607440948\n",
      "283 0.08355295658111572\n",
      "284 0.07989700883626938\n",
      "285 0.07640066742897034\n",
      "286 0.07304032146930695\n",
      "287 0.06984824687242508\n",
      "288 0.06679058074951172\n",
      "289 0.06385719776153564\n",
      "290 0.06105247884988785\n",
      "291 0.05839603394269943\n",
      "292 0.05584396794438362\n",
      "293 0.053401879966259\n",
      "294 0.05107516795396805\n",
      "295 0.04885739088058472\n",
      "296 0.04673045128583908\n",
      "297 0.04469609260559082\n",
      "298 0.04274005442857742\n",
      "299 0.0408705435693264\n",
      "300 0.039099641144275665\n",
      "301 0.03739367052912712\n",
      "302 0.03577357158064842\n",
      "303 0.03420868143439293\n",
      "304 0.03274018317461014\n",
      "305 0.031315892934799194\n",
      "306 0.02994130738079548\n",
      "307 0.028644222766160965\n",
      "308 0.027399523183703423\n",
      "309 0.02622014842927456\n",
      "310 0.02507798746228218\n",
      "311 0.02398662455379963\n",
      "312 0.02294779196381569\n",
      "313 0.02194933034479618\n",
      "314 0.021007558330893517\n",
      "315 0.02009359933435917\n",
      "316 0.01922893524169922\n",
      "317 0.018400123342871666\n",
      "318 0.017603674903512\n",
      "319 0.016845839098095894\n",
      "320 0.016126172617077827\n",
      "321 0.015431123785674572\n",
      "322 0.014761384576559067\n",
      "323 0.014139310456812382\n",
      "324 0.013530353084206581\n",
      "325 0.01295502483844757\n",
      "326 0.012393660843372345\n",
      "327 0.01187041774392128\n",
      "328 0.011375771835446358\n",
      "329 0.010886083357036114\n",
      "330 0.010420577600598335\n",
      "331 0.009976251050829887\n",
      "332 0.009550645016133785\n",
      "333 0.009148326702415943\n",
      "334 0.00876391027122736\n",
      "335 0.008400406688451767\n",
      "336 0.008044593967497349\n",
      "337 0.007703159004449844\n",
      "338 0.007386564742773771\n",
      "339 0.007075881119817495\n",
      "340 0.006788549013435841\n",
      "341 0.006504075136035681\n",
      "342 0.00623735599219799\n",
      "343 0.005978473927825689\n",
      "344 0.005732771009206772\n",
      "345 0.005498889368027449\n",
      "346 0.005273943301290274\n",
      "347 0.005054644774645567\n",
      "348 0.004856007173657417\n",
      "349 0.004658706020563841\n",
      "350 0.004469751846045256\n",
      "351 0.004288507625460625\n",
      "352 0.004114939831197262\n",
      "353 0.003955569583922625\n",
      "354 0.0037954829167574644\n",
      "355 0.00364454067312181\n",
      "356 0.0035069375298917294\n",
      "357 0.003367440076544881\n",
      "358 0.0032348190434277058\n",
      "359 0.0031101431231945753\n",
      "360 0.002990220906212926\n",
      "361 0.002877189079299569\n",
      "362 0.0027635388541966677\n",
      "363 0.002661315957084298\n",
      "364 0.0025591300800442696\n",
      "365 0.0024615530855953693\n",
      "366 0.002370379865169525\n",
      "367 0.002280936110764742\n",
      "368 0.002197590656578541\n",
      "369 0.0021176317241042852\n",
      "370 0.002041010418906808\n",
      "371 0.001966211711987853\n",
      "372 0.0018957192078232765\n",
      "373 0.001831058762036264\n",
      "374 0.001765434630215168\n",
      "375 0.0016997945494949818\n",
      "376 0.0016420520842075348\n",
      "377 0.0015840628184378147\n",
      "378 0.001526969252154231\n",
      "379 0.0014733935240656137\n",
      "380 0.0014232106041163206\n",
      "381 0.0013763091992586851\n",
      "382 0.00132784154266119\n",
      "383 0.001284297788515687\n",
      "384 0.0012405016459524632\n",
      "385 0.001198789570480585\n",
      "386 0.0011606195475906134\n",
      "387 0.0011225147172808647\n",
      "388 0.0010843953350558877\n",
      "389 0.0010494032176211476\n",
      "390 0.001015336369164288\n",
      "391 0.0009828838519752026\n",
      "392 0.000952210568357259\n",
      "393 0.0009212807053700089\n",
      "394 0.0008927379385568202\n",
      "395 0.0008649785304442048\n",
      "396 0.0008369560237042606\n",
      "397 0.0008131564827635884\n",
      "398 0.0007868419052101672\n",
      "399 0.0007632048800587654\n",
      "400 0.0007403172203339636\n",
      "401 0.0007199152023531497\n",
      "402 0.0006981990300118923\n",
      "403 0.0006777477101422846\n",
      "404 0.0006572742131538689\n",
      "405 0.0006380112026818097\n",
      "406 0.0006198218325152993\n",
      "407 0.0006029895739629865\n",
      "408 0.0005859028897248209\n",
      "409 0.0005686889635398984\n",
      "410 0.0005537274410016835\n",
      "411 0.0005374585161916912\n",
      "412 0.0005221781902946532\n",
      "413 0.000507616379763931\n",
      "414 0.0004942116211168468\n",
      "415 0.0004810411192011088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416 0.0004680791462305933\n",
      "417 0.0004555700288619846\n",
      "418 0.00044283716124482453\n",
      "419 0.0004308807256165892\n",
      "420 0.0004197129455860704\n",
      "421 0.0004094038449693471\n",
      "422 0.0003987450909335166\n",
      "423 0.0003876851696986705\n",
      "424 0.00037719920510426164\n",
      "425 0.0003684620023705065\n",
      "426 0.00035879481583833694\n",
      "427 0.00034950030385516584\n",
      "428 0.0003407062613405287\n",
      "429 0.00033312157029286027\n",
      "430 0.00032627591281197965\n",
      "431 0.0003183223307132721\n",
      "432 0.0003100494504906237\n",
      "433 0.00030293010058812797\n",
      "434 0.00029629163327626884\n",
      "435 0.00028917062445543706\n",
      "436 0.000282198452623561\n",
      "437 0.0002765730023384094\n",
      "438 0.0002694855793379247\n",
      "439 0.00026313646230846643\n",
      "440 0.000256833533057943\n",
      "441 0.00025161702069453895\n",
      "442 0.00024545230553485453\n",
      "443 0.0002403398248134181\n",
      "444 0.00023494652123190463\n",
      "445 0.00022974605963099748\n",
      "446 0.00022465288930106908\n",
      "447 0.0002197691355831921\n",
      "448 0.0002146622573491186\n",
      "449 0.00021044549066573381\n",
      "450 0.0002058181562460959\n",
      "451 0.00020202450104989111\n",
      "452 0.00019731292559299618\n",
      "453 0.00019337017147336155\n",
      "454 0.00018898501002695411\n",
      "455 0.00018521926540415734\n",
      "456 0.00018230077694170177\n",
      "457 0.00017832811863627285\n",
      "458 0.000174959292053245\n",
      "459 0.00017113734793383628\n",
      "460 0.0001679146953392774\n",
      "461 0.0001646594755584374\n",
      "462 0.0001614375796634704\n",
      "463 0.00015860963321756572\n",
      "464 0.00015574794088024646\n",
      "465 0.00015250845171976835\n",
      "466 0.00014934365754015744\n",
      "467 0.00014687828661408275\n",
      "468 0.00014368580013979226\n",
      "469 0.00014109273615758866\n",
      "470 0.00013875575677957386\n",
      "471 0.0001362027833238244\n",
      "472 0.00013351175584830344\n",
      "473 0.0001312483218498528\n",
      "474 0.00012867605255451053\n",
      "475 0.00012656234321184456\n",
      "476 0.00012481550220400095\n",
      "477 0.00012169562978670001\n",
      "478 0.00011954149522352964\n",
      "479 0.00011740770423784852\n",
      "480 0.00011574813106562942\n",
      "481 0.00011366156104486436\n",
      "482 0.00011156053369631991\n",
      "483 0.00010963394015561789\n",
      "484 0.00010779089643619955\n",
      "485 0.00010634434875100851\n",
      "486 0.00010449212277308106\n",
      "487 0.00010272228246321902\n",
      "488 0.00010104574903380126\n",
      "489 9.959148155758157e-05\n",
      "490 9.791142656467855e-05\n",
      "491 9.646370017435402e-05\n",
      "492 9.509657684247941e-05\n",
      "493 9.33739502215758e-05\n",
      "494 9.178353502647951e-05\n",
      "495 9.023829625220969e-05\n",
      "496 8.900257671484724e-05\n",
      "497 8.746157982386649e-05\n",
      "498 8.597721171099693e-05\n",
      "499 8.477608935208991e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
